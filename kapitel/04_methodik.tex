\chapter{Daten und Methodik der Evaluation}

\section{Identifizierung von Use Cases}

\section{Evaluation eines Retrieval-Augmented-Generation-Chatbots}

\subsection{Stand der Forschung}

Der Stand der Forschung zur Evaluation von Retrieval-Augmented Generation (RAG) ist durch eine zunehmende methodische Ausdifferenzierung geprägt, die technische Qualitätsdimensionen ebenso einbezieht wie nutzerbezogene und domänenspezifische Anforderungen. Ausgangspunkt ist die RAG-Architektur, die parametrisierte Sprachmodelle mit externem, nicht-parametrischem Wissen koppelt, um faktische Genauigkeit zu erhöhen und Halluzinationen zu reduzieren. Diese Hybridität erzwingt eine doppelte Evaluationslogik: Einerseits ist die Qualität des Retrievals zu beurteilen (z.\,B. ob relevante Evidenz gefunden und bereitgestellt wird), andererseits die Generationsleistung unter Nutzung des bereitgestellten Kontexts (z.\,B. ob Antworten korrekt, vollständig und im Kontext verankert sind) \cite{Lewis2021RAG}.

Eine zentrale Herausforderung besteht darin, dass RAG-Systeme hybride Fehlerquellen besitzen: Fehler können durch irrelevante oder falsche Dokumente im Retrieval entstehen, durch unzureichende Integration mehrerer Passagen oder durch Halluzinationen trotz verfügbarer Evidenz. Entsprechend argumentieren neuere Survey-Arbeiten, dass Evaluation entlang der Pipeline-Phasen strukturiert werden sollte, um die Schnittstellen zwischen Retrieval und Generation explizit zu berücksichtigen \cite{Yu2025SurveyRAG}.

Im Zuge dieser Entwicklung vollzieht sich eine Abkehr von klassischen NLG-Metriken wie BLEU oder ROUGE, da diese primär Oberflächenähnlichkeit messen und Kontexttreue nicht adäquat erfassen \cite{Roychowdhury2024RAGAS}. Stattdessen rücken kontextbezogene Qualitätsdimensionen in den Fokus, insbesondere \emph{Faithfulness} und faktische Korrektheit, die prüfen, ob generierte Aussagen durch den bereitgestellten Kontext gestützt werden \cite{Roychowdhury2024RAGAS}.

Ein zentraler methodischer Fortschritt liegt in automatisierten, referenzarmen Evaluationsframeworks. RAGAS nutzt LLMs als Bewertungsinstanz und operationalisiert Metriken wie \emph{Faithfulness} und Relevanzdimensionen, wodurch eine skalierbare Evaluation auch ohne umfangreiche Ground-Truth-Datensätze möglich wird \cite{Es2023RAGAS}. Domänenspezifische Analysen zeigen jedoch, dass solche Metriken kritisch validiert werden müssen. Roychowdhury et al.\ weisen nach, dass insbesondere \emph{Faithfulness} und faktische Korrektheit vergleichsweise gut mit Expertenurteilen korrelieren, während andere Metriken sensitiv auf Retrieval-Fehler oder Embedding-Varianten reagieren können \cite{Roychowdhury2024RAGAS}. Ergänzend argumentieren Wang et al., dass starke LLMs als Evaluatoren in geschlossenen, faktischen Settings hohe Übereinstimmungen mit menschlichen Bewertungen erreichen, jedoch menschliche Evaluation nicht vollständig ersetzen \cite{Wang2024LLMEvaluation}.

Im Bildungskontext erweitert sich die Evaluationsperspektive um pädagogische Zielgrößen. Übersichtsarbeiten zeigen, dass RAG-Chatbots insbesondere zur Halluzinationsreduktion und curricularen Einbettung eingesetzt werden \cite{Swacha2025RAGEducation}. Li et al.\ betonen, dass neben Faktentreue auch Aktualität, Vollständigkeit und didaktische Angemessenheit entscheidend sind \cite{Li2025RAGEducationSurvey}. Empirische Studien wie die zu MoodleBot kombinieren technische Genauigkeitsmessungen mit Akzeptanzanalysen auf Basis des Technology Acceptance Model (TAM) und erfassen sowohl inhaltliche Qualität als auch wahrgenommene Nützlichkeit und Bedienbarkeit \cite{Neumann2024MoodleBot}.

Eine weitere relevante Evaluationslinie stellen paarweise Blindvergleiche nach dem Arena-Prinzip dar. Zwei Systeme beantworten dieselbe Anfrage, und Rater bewerten ohne Kenntnis der Systemidentität die überzeugendere Antwort. Dieses Vorgehen ist besonders geeignet für generative Systeme, da offene Antworten nicht immer eindeutig gegen eine Ground Truth gemessen werden können. Während Metriken wie \emph{Faithfulness} oder faktische Korrektheit Kontexttreue und inhaltliche Richtigkeit quantifizieren \cite{Roychowdhury2024RAGAS}, erfassen Blindtests zusätzlich wahrgenommene Verständlichkeit, Struktur, Relevanzgewichtung und didaktische Qualität. Gerade im Bildungsbereich, in dem Akzeptanz und wahrgenommener Nutzen zentrale Erfolgsfaktoren sind \cite{Neumann2024MoodleBot}, ermöglicht dieses Design eine praxisnahe Bewertung der Gesamtqualität zweier RAG-Systeme.

Neben Qualitäts- und Akzeptanzdimensionen gewinnt zunehmend auch die wirtschaftliche Perspektive an Bedeutung. Mehrere Arbeiten verweisen auf Fragen der technischen Skalierbarkeit und des Ressourcenverbrauchs, ohne diese systematisch zu quantifizieren \cite{Neumann2024MoodleBot}. Für produktive LMS-Integrationen sind jedoch Token-Verbrauch, API-Kosten, Antwortlatenz und Infrastrukturaufwand zentrale Kenngrößen, da sie die nachhaltige Einsetzbarkeit maßgeblich beeinflussen. Eine umfassende Evaluation sollte daher auch ökonomische Indikatoren berücksichtigen.

Zusammenfassend lässt sich der Forschungsstand in drei Entwicklungslinien bündeln: Erstens die Ausdifferenzierung technischer Metriken entlang von Retrieval- und Generationsdimensionen mit Fokus auf Kontexttreue und Robustheit \cite{Yu2025SurveyRAG}. Zweitens die Etablierung automatisierter, LLM-gestützter Evaluationsframeworks wie RAGAS zur skalierbaren Qualitätsmessung, deren Ergebnisse jedoch domänenspezifisch validiert werden müssen \cite{Es2023RAGAS,Roychowdhury2024RAGAS}. Drittens die Integration menschlicher Urteile – sowohl nutzer- als auch expertengestützt – sowie die Berücksichtigung ökonomischer Faktoren im Anwendungskontext \cite{Neumann2024MoodleBot}. Vor diesem Hintergrund ist eine kombinierte Evaluationsstrategie, die automatisierte Metriken, Blindtests und wirtschaftliche Analysen zusammenführt, methodisch gut begründet und entspricht dem aktuellen Stand der Forschung.

\subsection{Evaluationskonzept}

\subsubsection{Umsetzung RAGAS}

\textbf{Methodischer Rahmen}

Zur automatisierten Evaluation des Retrieval-Augmented-Generation-Systems wurde das Framework RAGAS in der Version 0.3.8 eingesetzt. RAGAS ist ein referenzfreies Evaluationsframework für RAG-Systeme, das verschiedene Qualitätsdimensionen automatisiert bewertet \cite{es2025ragas}. Es ermöglicht die quantitative Bewertung von Antwort- und Retrieval-Qualität, ohne dass manuell erstellte Referenzantworten erforderlich sind.

Die Evaluation wurde als Single-Turn-Setting umgesetzt. Jede Frage wurde unabhängig von vorherigen Interaktionen bewertet, um eine kontrollierte und vergleichbare Analyse der Systemleistung zu gewährleisten. Dieses Vorgehen ist mit der Dokumentation von RAGAS konsistent \cite{ragasdocs}.

Die Evaluationsfragen wurden kuratiert und nicht zufällig generiert. Ziel war es, typische Nutzungsszenarien der Plattform systematisch abzubilden. Die 60 Fragen decken Wissensfragen zu KI-Inhalten, kursbezogene Anfragen, technische Supportfragen sowie Fragen zur Plattformfunktionalität ab.

\bigskip
\textbf{Technische Implementierung und Metrikdefinition}

Die Implementierung orientiert sich an der von RAGAS vorgesehenen Struktur \texttt{SingleTurnSample} \cite{ragasdocs}. Für jede Evaluationsinstanz werden die Nutzerfrage, die generierte Antwort sowie die tatsächlich verwendeten Kontextsegmente an das Framework übergeben.

Als Metriken wurden \textit{Answer Relevancy}, \textit{Context Relevance} und \textit{Faithfulness} eingesetzt \cite{es2025ragas}.

\textit{Answer Relevancy} misst die inhaltliche Passung zwischen Frage und Antwort.  
\textit{Context Relevance} bewertet die Qualität des Retrievals.  
\textit{Faithfulness} überprüft, ob die Antwort durch die bereitgestellten Kontexte gestützt ist.

Die Bewertung erfolgte mit \texttt{gpt-4o} über den \texttt{LangchainLLMWrapper}. Für semantische Ähnlichkeitsberechnungen wurden Embeddings des Modells \texttt{text-embedding-3-small} verwendet. Die Temperatur wurde auf 0 gesetzt.

\bigskip
\textbf{Evaluation der bestehenden und der neuen Version}

In der bestehenden Version basiert die RAG-Pipeline auf einer linearen Architektur. Die extrahierten Kontexte wurden gemeinsam mit der generierten Antwort an RAGAS übergeben.

Die neue Version verwendet eine Klassifikation der Anfrage sowie Single-Hop- oder Multi-Hop-Verfahren. Komplexe Anfragen werden in Teilfragen zerlegt, parallel verarbeitet und anschließend dedupliziert und rerankt.

In beiden Versionen wurden ausschließlich die final ausgewählten Kontexte als \texttt{retrieved\_contexts} an RAGAS übergeben.

\bigskip
\textbf{Mehrfachausführung und Datenaggregation}

Jede Frage wurde fünfmal beantwortet und bewertet. Die Metrikwerte wurden gespeichert und zu Durchschnittswerten aggregiert.

Alle Evaluationsdaten wurden in einer strukturierten JSON-Datei dokumentiert. Ergänzend wurde ein Prüfmechanismus implementiert, der potenziell ungestützte Antworten identifiziert.