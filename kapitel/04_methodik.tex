\chapter{Daten und Methodik der Evaluation}

\section{Identifizierung von Use Cases}

\section{Evaluation eines Retrieval-Augmented-Generation-Chatbots}

\subsection{Stand der Forschung}

Der Stand der Forschung zur Evaluation von Retrieval-Augmented Generation (RAG) ist durch eine zunehmende methodische Ausdifferenzierung geprägt, die technische Qualitätsdimensionen ebenso einbezieht wie nutzerbezogene und domänenspezifische Anforderungen. Ausgangspunkt ist die RAG-Architektur, die parametrisierte Sprachmodelle mit externem, nicht-parametrischem Wissen koppelt, um faktische Genauigkeit zu erhöhen und Halluzinationen zu reduzieren. Diese Hybridität erzwingt eine doppelte Evaluationslogik: Einerseits ist die Qualität des Retrievals zu beurteilen (z.\,B. ob relevante Evidenz gefunden und bereitgestellt wird), andererseits die Generationsleistung unter Nutzung des bereitgestellten Kontexts (z.\,B. ob Antworten korrekt, vollständig und im Kontext verankert sind) \cite{Lewis2021RAG}.

Eine zentrale Herausforderung besteht darin, dass RAG-Systeme hybride Fehlerquellen besitzen: Fehler können durch irrelevante oder falsche Dokumente im Retrieval entstehen, durch unzureichende Integration mehrerer Passagen oder durch Halluzinationen trotz verfügbarer Evidenz. Entsprechend argumentieren neuere Survey-Arbeiten, dass Evaluation entlang der Pipeline-Phasen strukturiert werden sollte, um die Schnittstellen zwischen Retrieval und Generation explizit zu berücksichtigen \cite{Yu2025SurveyRAG}.

Im Zuge dieser Entwicklung vollzieht sich eine Abkehr von klassischen NLG-Metriken wie BLEU oder ROUGE, da diese primär Oberflächenähnlichkeit messen und Kontexttreue nicht adäquat erfassen \cite{Roychowdhury2024RAGAS}. Stattdessen rücken kontextbezogene Qualitätsdimensionen in den Fokus, insbesondere \emph{Faithfulness} und faktische Korrektheit, die prüfen, ob generierte Aussagen durch den bereitgestellten Kontext gestützt werden \cite{Roychowdhury2024RAGAS}.

Ein zentraler methodischer Fortschritt liegt in automatisierten, referenzarmen Evaluationsframeworks. RAGAS nutzt LLMs als Bewertungsinstanz und operationalisiert Metriken wie \emph{Faithfulness} und Relevanzdimensionen, wodurch eine skalierbare Evaluation auch ohne umfangreiche Ground-Truth-Datensätze möglich wird \cite{Es2023RAGAS}. Domänenspezifische Analysen zeigen jedoch, dass solche Metriken kritisch validiert werden müssen. Roychowdhury et al.\ weisen nach, dass insbesondere \emph{Faithfulness} und faktische Korrektheit vergleichsweise gut mit Expertenurteilen korrelieren, während andere Metriken sensitiv auf Retrieval-Fehler oder Embedding-Varianten reagieren können \cite{Roychowdhury2024RAGAS}. Ergänzend argumentieren Wang et al., dass starke LLMs als Evaluatoren in geschlossenen, faktischen Settings hohe Übereinstimmungen mit menschlichen Bewertungen erreichen, jedoch menschliche Evaluation nicht vollständig ersetzen \cite{Wang2024LLMEvaluation}.

Im Bildungskontext erweitert sich die Evaluationsperspektive um pädagogische Zielgrößen. Übersichtsarbeiten zeigen, dass RAG-Chatbots insbesondere zur Halluzinationsreduktion und curricularen Einbettung eingesetzt werden \cite{Swacha2025RAGEducation}. Li et al.\ betonen, dass neben Faktentreue auch Aktualität, Vollständigkeit und didaktische Angemessenheit entscheidend sind \cite{Li2025RAGEducationSurvey}. Empirische Studien wie die zu MoodleBot kombinieren technische Genauigkeitsmessungen mit Akzeptanzanalysen auf Basis des Technology Acceptance Model (TAM) und erfassen sowohl inhaltliche Qualität als auch wahrgenommene Nützlichkeit und Bedienbarkeit \cite{Neumann2024MoodleBot}.

Eine weitere relevante Evaluationslinie stellen paarweise Blindvergleiche nach dem Arena-Prinzip dar. Zwei Systeme beantworten dieselbe Anfrage, und Rater bewerten ohne Kenntnis der Systemidentität die überzeugendere Antwort. Dieses Vorgehen ist besonders geeignet für generative Systeme, da offene Antworten nicht immer eindeutig gegen eine Ground Truth gemessen werden können. Während Metriken wie \emph{Faithfulness} oder faktische Korrektheit Kontexttreue und inhaltliche Richtigkeit quantifizieren \cite{Roychowdhury2024RAGAS}, erfassen Blindtests zusätzlich wahrgenommene Verständlichkeit, Struktur, Relevanzgewichtung und didaktische Qualität. Gerade im Bildungsbereich, in dem Akzeptanz und wahrgenommener Nutzen zentrale Erfolgsfaktoren sind \cite{Neumann2024MoodleBot}, ermöglicht dieses Design eine praxisnahe Bewertung der Gesamtqualität zweier RAG-Systeme.

Neben Qualitäts- und Akzeptanzdimensionen gewinnt zunehmend auch die wirtschaftliche Perspektive an Bedeutung. Mehrere Arbeiten verweisen auf Fragen der technischen Skalierbarkeit und des Ressourcenverbrauchs, ohne diese systematisch zu quantifizieren \cite{Neumann2024MoodleBot}. Für produktive LMS-Integrationen sind jedoch Token-Verbrauch, API-Kosten, Antwortlatenz und Infrastrukturaufwand zentrale Kenngrößen, da sie die nachhaltige Einsetzbarkeit maßgeblich beeinflussen. Eine umfassende Evaluation sollte daher auch ökonomische Indikatoren berücksichtigen.

Zusammenfassend lässt sich der Forschungsstand in drei Entwicklungslinien bündeln: Erstens die Ausdifferenzierung technischer Metriken entlang von Retrieval- und Generationsdimensionen mit Fokus auf Kontexttreue und Robustheit \cite{Yu2025SurveyRAG}. Zweitens die Etablierung automatisierter, LLM-gestützter Evaluationsframeworks wie RAGAS zur skalierbaren Qualitätsmessung, deren Ergebnisse jedoch domänenspezifisch validiert werden müssen \cite{Es2023RAGAS,Roychowdhury2024RAGAS}. Drittens die Integration menschlicher Urteile – sowohl nutzer- als auch expertengestützt – sowie die Berücksichtigung ökonomischer Faktoren im Anwendungskontext \cite{Neumann2024MoodleBot}. Vor diesem Hintergrund ist eine kombinierte Evaluationsstrategie, die automatisierte Metriken, Blindtests und wirtschaftliche Analysen zusammenführt, methodisch gut begründet und entspricht dem aktuellen Stand der Forschung.

\subsection{Evaluationskonzept}

\subsubsection{Blindtest: Experten}

Zur vergleichenden Evaluation der beiden RAG-Systeme wurde ein expertenbasierter Blindtest im A/B-Design durchgeführt. Methodisch orientiert sich das Vorgehen am Arena-Prinzip, bei dem zwei Systemantworten auf denselben Prompt parallel präsentiert werden, ohne dass die Bewertenden wissen, welches System welche Antwort erzeugt hat. Dieses Design reduziert Erwartungs- und Confirmation-Bias und ist insbesondere für generative Systeme geeignet, deren Qualität nicht ausschließlich anhand einer eindeutig definierten Ground Truth beurteilt werden kann. Während automatisierte Metriken wie \emph{Faithfulness} oder faktische Korrektheit die kontextuelle Stützung und inhaltliche Konsistenz quantifizieren \cite{Roychowdhury2024RAGAS}, erlaubt ein Blindvergleich die Bewertung wahrgenommener Gesamtqualität im direkten Systemvergleich.

\begin{figure}[h]
    \centering
    \fbox{\parbox[c][6cm][c]{0.9\textwidth}{\centering Platzhalter für Abbildung: Arena-Screenshot mit A/B-Systemvergleich}}
    \caption{Arena-Setup im Experten-Blindtest: Zwei Systemantworten (A/B) werden parallel im identischen Interface dargestellt.}
    \label{fig:arena}
\end{figure}

Die Untersuchung wurde als moderiertes Einzelinterview durchgeführt (30--60 Minuten pro Person). Insgesamt nahmen 14 Expert*innen teil, darunter Autor*innen von E-Learning-Inhalten, Lehrkräfte sowie Fachpersonen aus dem KI-Campus-Umfeld, aus der angewandten Forschung (Fraunhofer) und aus dem universitären Kontext (Freie Universität Berlin). Diese Zusammensetzung gewährleistet sowohl fachliche als auch didaktische Expertise und erlaubt eine fundierte Beurteilung inhaltlicher Korrektheit, Kontextintegration und Lernförderlichkeit. Die Relevanz solcher mehrdimensionaler Qualitätskriterien im Bildungskontext wird auch in der Literatur zu RAG in Educational Settings hervorgehoben \cite{Li2025RAGEducationSurvey,Swacha2025RAGEducation}. Zudem zeigen Studien zur Nutzung generativer KI im Unterricht, dass didaktische Angemessenheit und die Förderung kritischer Bewertungskompetenz zentrale Qualitätsdimensionen darstellen \cite{Oates2025ChatGPTClassroom}.

\textbf{Bewertungsinstrument}

Die Evaluation erfolgte mittels standardisiertem Fragebogen (Likert-Skala, 1 = trifft nicht zu, 5 = trifft voll zu) sowie begleitender Think-Aloud-Methode. Die Bewertungsdimensionen wurden wie folgt operationalisiert:

\begin{table}[h]
\centering
\caption{Bewertungsdimensionen des Experten-Blindtests}
\label{tab:experten_kriterien}
\begin{tabular}{p{1cm} p{3.5cm} p{7cm}}
\hline
\textbf{Kürzel} & \textbf{Dimension} & \textbf{Bewertungsstatement} \\
\hline
I1 & Inhaltliche Korrektheit & Die Antwort ist fachlich korrekt und enthält keine sachlichen Fehler. \\
I2 & Vollständigkeit & Die Antwort deckt die wesentlichen Aspekte der Frage vollständig ab. \\
I3 & Relevanz & Die Antwort geht präzise auf die gestellte Frage ein und enthält keine irrelevanten Informationen. \\
I4 & Kontextintegration / Kohärenz & Die Antwort integriert relevante Kontextinformationen schlüssig und ist in sich konsistent. \\
I5 & Verständlichkeit / Struktur & Die Antwort ist klar strukturiert, gut verständlich formuliert und sprachlich angemessen. \\
I6 & Lernförderlichkeit / Didaktik & Die Antwort unterstützt das Verständnis des Themas und fördert den Lernprozess. \\
\hline
\end{tabular}
\end{table}

Die Kombination aus quantitativer Skalenbewertung und qualitativer Think-Aloud-Methode ermöglicht eine differenzierte Analyse. Während die Likert-Skalen systematische Vergleiche der beiden Systeme über mehrere Qualitätsdimensionen hinweg erlauben, liefert die verbale Begleitreflexion Einblick in Entscheidungslogiken, Unsicherheiten und Bewertungsmaßstäbe der Expert*innen. Gerade im Bildungskontext ist diese Kombination bedeutsam, da generative KI-Systeme nicht allein anhand faktischer Richtigkeit bewertet werden, sondern insbesondere hinsichtlich ihrer didaktischen Angemessenheit, Strukturierungsleistung und Unterstützung reflektierter Wissensaneignung \cite{Oates2025ChatGPTClassroom,Neumann2024MoodleBot}.

Der Experten-Blindtest ergänzt somit automatisierte RAG-Metriken um eine fachlich-didaktische Validierungsebene. Während quantitative Scores Aussagen über Kontexttreue und faktische Konsistenz erlauben \cite{Roychowdhury2024RAGAS}, erfasst das Arena-Design die wahrgenommene Gesamtqualität im direkten Vergleich – eine zentrale Voraussetzung für die Bewertung konkurrierender RAG-Systeme im Bildungsbereich.



\subsubsection{Umsetzung RAGAS}

\textbf{Methodischer Rahmen}

Zur automatisierten Evaluation des Retrieval-Augmented-Generation-Systems wurde das Framework RAGAS in der Version 0.3.8 eingesetzt. RAGAS ist ein referenzfreies Evaluationsframework für RAG-Systeme, das verschiedene Qualitätsdimensionen automatisiert bewertet \cite{es2025ragas}. Es ermöglicht die quantitative Bewertung von Antwort- und Retrieval-Qualität, ohne dass manuell erstellte Referenzantworten erforderlich sind.

Die Evaluation wurde als Single-Turn-Setting umgesetzt. Jede Frage wurde unabhängig von vorherigen Interaktionen bewertet, um eine kontrollierte und vergleichbare Analyse der Systemleistung zu gewährleisten. Dieses Vorgehen ist mit der Dokumentation von RAGAS konsistent \cite{ragasdocs}.

Die Evaluationsfragen wurden kuratiert und nicht zufällig generiert. Ziel war es, typische Nutzungsszenarien der Plattform systematisch abzubilden. Die 60 Fragen decken Wissensfragen zu KI-Inhalten, kursbezogene Anfragen, technische Supportfragen sowie Fragen zur Plattformfunktionalität ab.

\bigskip
\textbf{Technische Implementierung und Metrikdefinition}

Die Implementierung orientiert sich an der von RAGAS vorgesehenen Struktur \texttt{SingleTurnSample} \cite{ragasdocs}. Für jede Evaluationsinstanz werden die Nutzerfrage, die generierte Antwort sowie die tatsächlich verwendeten Kontextsegmente an das Framework übergeben.

Als Metriken wurden \textit{Answer Relevancy}, \textit{Context Relevance} und \textit{Faithfulness} eingesetzt \cite{es2025ragas}.

\textit{Answer Relevancy} misst die inhaltliche Passung zwischen Frage und Antwort.  
\textit{Context Relevance} bewertet die Qualität des Retrievals.  
\textit{Faithfulness} überprüft, ob die Antwort durch die bereitgestellten Kontexte gestützt ist.

Die Bewertung erfolgte mit \texttt{gpt-4o} über den \texttt{LangchainLLMWrapper}. Für semantische Ähnlichkeitsberechnungen wurden Embeddings des Modells \texttt{text-embedding-3-small} verwendet. Die Temperatur wurde auf 0 gesetzt.

\bigskip
\textbf{Evaluation der bestehenden und der neuen Version}

In der bestehenden Version basiert die RAG-Pipeline auf einer linearen Architektur. Die extrahierten Kontexte wurden gemeinsam mit der generierten Antwort an RAGAS übergeben.

Die neue Version verwendet eine Klassifikation der Anfrage sowie Single-Hop- oder Multi-Hop-Verfahren. Komplexe Anfragen werden in Teilfragen zerlegt, parallel verarbeitet und anschließend dedupliziert und rerankt.

In beiden Versionen wurden ausschließlich die final ausgewählten Kontexte als \texttt{retrieved\_contexts} an RAGAS übergeben.

\bigskip
\textbf{Mehrfachausführung und Datenaggregation}

Jede Frage wurde fünfmal beantwortet und bewertet. Die Metrikwerte wurden gespeichert und zu Durchschnittswerten aggregiert.

Alle Evaluationsdaten wurden in einer strukturierten JSON-Datei dokumentiert. Ergänzend wurde ein Prüfmechanismus implementiert, der potenziell ungestützte Antworten identifiziert.

\section{Weiterentwicklung eines Retrieval-Augmented-Generation-Chatbots}

\subsection{Anforderungsanlyse}
Die Weiterentwicklung eines RAG-basierten Chatbots ist ebenso wie dessen Evaluation keine triviale Aufgabe. Um eine zielgerichtete Arbeit sicherzustellen, bedarf es einer systematischen Erfassung der Anforderungen, sodass darauf aufbauend konkrete technische Implementierungsschritte definiert und umgesetzt werden können. In diesem Abschnitt wird die Anforderungsanalyse beschrieben, die als Grundlage für die Weiterentwicklung des Chatbots dient. Zuerst werden allgemeine Anforderungen an RAG-basierte Chatbots auf Basis einer Literaturrecherche identifiziert. Der Fokus liegt hier bereits auf dem Bildungskontext. Darauffolgend werden spezifische Anforderungen für die Weiterentwicklung des Chatbots auf Basis der zentralen Stakeholder definiert. Dazu gehören die Nutzer der Plattform, die den Chatbot verwenden, sowie der KI-Campus als Betreiber der Plattform. Damit wird sichergestellt, dass die Weiterentwicklung des Chatbots nicht nur auf theoretischen Überlegungen basiert, sondern auch die praktischen Bedürfnisse und Erwartungen der Nutzer und des Betreibers berücksichtigt.

\subsubsection{Stand der Forschung}
??

\subsubsection{Stakeholder}
Der Einbezug der Stakeholder erfolgt primär durch das Auswerten einer ersten Phase von User Test. Diese wurden im Februar 2025 nach X jährigen Bestehen des Chatbots druchgeführt.
INFOS ZU USER TEST ERFRAGEN
Die Ergebnisse der User Tests ist in Tabelle \ref{tab:usertests}

\begin{table}[htbp]
\centering
\renewcommand{\arraystretch}{1.2}

\begin{tabular}{|>{\raggedright\arraybackslash}m{0.3\textwidth}|>{\raggedright\arraybackslash}m{0.3\textwidth}|>{\raggedright\arraybackslash}m{0.3\textwidth}|}
\hline
\multicolumn{1}{|c|}{\textbf{Hoch}} &
\multicolumn{1}{c|}{\textbf{Mittel}} &
\multicolumn{1}{c|}{\textbf{Niedrig}} \\
\hline
Langsames Antworten & Abbrechen von  Chatanfragen fehlt & Simples Interface \\
\hline
Ausbaufähiges Zitieren & Keine personalisierten Antworten & Level der User unbeachtet \\
\hline
Antwortgenauigkeit \& -qualität & Editieren von Chatanfragen fehlt & Zwei Antworten generieren (siehe ChatGPT)? \\
\hline
Mehrsprachiger Support unzureichend & Keine informellen Lernabfragen & Rolle des Chatbots klar benennen \\
\hline
Unzureichende Persistenz der Konversationen & Beispiele und Use Cases in Antwort einbinden & Kein Hinweis auf Funktionalität Chatbots \\
\hline
User Feedback einbinden (RLHF) & Schlechte No-Answer Logik & Keine Exportfunktion der Chats \\
\hline
Größere Wissensbasis für Antwort & Schwache Outputformatierung & Antworten teils generisch und nicht kontextgetreu \\
\hline
Oberflächliche Zusammenfassungen & Multimodale Informationen nicht eingebunden & Keine KIC Community Info im Chatkontext \\
\hline
Follow-up Fragen fehlen &  &  \\
\hline
\end{tabular}
\caption{Festgestellte Schwachstellen nach Relevanz}
\label{tab:usertests}
\end{table}

\subsubsection{Identifizierte Systemanforderungen}
Auf Basis der durchgeführten Anforderungsanalyse wurden die in Abbildung \ref{fig:systemanforderungen} dargestellten Systemanforderungen für die Weiterentwicklung des RAG-basierten Chatbots identifiziert.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{systemanforderungen.png}
    \caption{Identifizierte Systemanforderungen}
    \label{fig:systemanforderungen}
\end{figure}

\subsection{Technische Umsetzung}

\subsubsection{Codeanalyse}

\subsubsection{Erweiterung der Ingestion-Pipeline}
Veit

\subsubsection{Umstellung der Logik}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{workflow_neu.png}
    \caption{Neuer Workflow des RAG-basierten Chatbots in LangGraph}
    \label{fig:worklow_neu}
\end{figure}

\subsubsection{Spezifische Anpassungen}

\subsubsection{Begründung ausgelassener Anforderungen}
