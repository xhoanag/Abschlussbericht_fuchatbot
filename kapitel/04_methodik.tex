\chapter{Daten und Methodik der Evaluation}

\section{Identifizierung von Use Cases}

\section{Evaluation eines Retrieval-Augmented-Generation-Chatbots}

\subsection{Stand der Forschung}

Der Stand der Forschung zur Evaluation von Retrieval-Augmented Generation (RAG) ist durch eine zunehmende methodische Ausdifferenzierung geprägt, die technische Qualitätsdimensionen ebenso einbezieht wie nutzerbezogene und domänenspezifische Anforderungen. Ausgangspunkt ist die RAG-Architektur, die parametrisierte Sprachmodelle mit externem, nicht-parametrischem Wissen koppelt, um faktische Genauigkeit zu erhöhen und Halluzinationen zu reduzieren. Diese Hybridität erzwingt eine doppelte Evaluationslogik: Einerseits ist die Qualität des Retrievals zu beurteilen (z.\,B. ob relevante Evidenz gefunden und bereitgestellt wird), andererseits die Generationsleistung unter Nutzung des bereitgestellten Kontexts (z.\,B. ob Antworten korrekt, vollständig und im Kontext verankert sind) \cite{Lewis2021RAG}.

Eine zentrale Herausforderung besteht darin, dass RAG-Systeme hybride Fehlerquellen besitzen: Fehler können durch irrelevante oder falsche Dokumente im Retrieval entstehen, durch unzureichende Integration mehrerer Passagen oder durch Halluzinationen trotz verfügbarer Evidenz. Entsprechend argumentieren neuere Survey-Arbeiten, dass Evaluation entlang der Pipeline-Phasen strukturiert werden sollte, um die Schnittstellen zwischen Retrieval und Generation explizit zu berücksichtigen \cite{Yu2025SurveyRAG}.

Im Zuge dieser Entwicklung vollzieht sich eine Abkehr von klassischen NLG-Metriken wie BLEU oder ROUGE, da diese primär Oberflächenähnlichkeit messen und Kontexttreue nicht adäquat erfassen \cite{Roychowdhury2024RAGAS}. Stattdessen rücken kontextbezogene Qualitätsdimensionen in den Fokus, insbesondere \emph{Faithfulness} und faktische Korrektheit, die prüfen, ob generierte Aussagen durch den bereitgestellten Kontext gestützt werden \cite{Roychowdhury2024RAGAS}.

Ein zentraler methodischer Fortschritt liegt in automatisierten, referenzarmen Evaluationsframeworks. RAGAS nutzt LLMs als Bewertungsinstanz und operationalisiert Metriken wie \emph{Faithfulness} und Relevanzdimensionen, wodurch eine skalierbare Evaluation auch ohne umfangreiche Ground-Truth-Datensätze möglich wird \cite{Es2023RAGAS}. Domänenspezifische Analysen zeigen jedoch, dass solche Metriken kritisch validiert werden müssen. Roychowdhury et al.\ weisen nach, dass insbesondere \emph{Faithfulness} und faktische Korrektheit vergleichsweise gut mit Expertenurteilen korrelieren, während andere Metriken sensitiv auf Retrieval-Fehler oder Embedding-Varianten reagieren können \cite{Roychowdhury2024RAGAS}. Ergänzend argumentieren Wang et al., dass starke LLMs als Evaluatoren in geschlossenen, faktischen Settings hohe Übereinstimmungen mit menschlichen Bewertungen erreichen, jedoch menschliche Evaluation nicht vollständig ersetzen \cite{Wang2024LLMEvaluation}.

Im Bildungskontext erweitert sich die Evaluationsperspektive um pädagogische Zielgrößen. Übersichtsarbeiten zeigen, dass RAG-Chatbots insbesondere zur Halluzinationsreduktion und curricularen Einbettung eingesetzt werden \cite{Swacha2025RAGEducation}. Li et al.\ betonen, dass neben Faktentreue auch Aktualität, Vollständigkeit und didaktische Angemessenheit entscheidend sind \cite{Li2025RAGEducationSurvey}. Empirische Studien wie die zu MoodleBot kombinieren technische Genauigkeitsmessungen mit Akzeptanzanalysen auf Basis des Technology Acceptance Model (TAM) und erfassen sowohl inhaltliche Qualität als auch wahrgenommene Nützlichkeit und Bedienbarkeit \cite{Neumann2024MoodleBot}.

Eine weitere relevante Evaluationslinie stellen paarweise Blindvergleiche nach dem Arena-Prinzip dar. Zwei Systeme beantworten dieselbe Anfrage, und Rater bewerten ohne Kenntnis der Systemidentität die überzeugendere Antwort. Dieses Vorgehen ist besonders geeignet für generative Systeme, da offene Antworten nicht immer eindeutig gegen eine Ground Truth gemessen werden können. Während Metriken wie \emph{Faithfulness} oder faktische Korrektheit Kontexttreue und inhaltliche Richtigkeit quantifizieren \cite{Roychowdhury2024RAGAS}, erfassen Blindtests zusätzlich wahrgenommene Verständlichkeit, Struktur, Relevanzgewichtung und didaktische Qualität. Gerade im Bildungsbereich, in dem Akzeptanz und wahrgenommener Nutzen zentrale Erfolgsfaktoren sind \cite{Neumann2024MoodleBot}, ermöglicht dieses Design eine praxisnahe Bewertung der Gesamtqualität zweier RAG-Systeme.

Neben Qualitäts- und Akzeptanzdimensionen gewinnt zunehmend auch die wirtschaftliche Perspektive an Bedeutung. Mehrere Arbeiten verweisen auf Fragen der technischen Skalierbarkeit und des Ressourcenverbrauchs, ohne diese systematisch zu quantifizieren \cite{Neumann2024MoodleBot}. Für produktive LMS-Integrationen sind jedoch Token-Verbrauch, API-Kosten, Antwortlatenz und Infrastrukturaufwand zentrale Kenngrößen, da sie die nachhaltige Einsetzbarkeit maßgeblich beeinflussen. Eine umfassende Evaluation sollte daher auch ökonomische Indikatoren berücksichtigen.

Zusammenfassend lässt sich der Forschungsstand in drei Entwicklungslinien bündeln: Erstens die Ausdifferenzierung technischer Metriken entlang von Retrieval- und Generationsdimensionen mit Fokus auf Kontexttreue und Robustheit \cite{Yu2025SurveyRAG}. Zweitens die Etablierung automatisierter, LLM-gestützter Evaluationsframeworks wie RAGAS zur skalierbaren Qualitätsmessung, deren Ergebnisse jedoch domänenspezifisch validiert werden müssen \cite{Es2023RAGAS,Roychowdhury2024RAGAS}. Drittens die Integration menschlicher Urteile – sowohl nutzer- als auch expertengestützt – sowie die Berücksichtigung ökonomischer Faktoren im Anwendungskontext \cite{Neumann2024MoodleBot}. Vor diesem Hintergrund ist eine kombinierte Evaluationsstrategie, die automatisierte Metriken, Blindtests und wirtschaftliche Analysen zusammenführt, methodisch gut begründet und entspricht dem aktuellen Stand der Forschung.

\subsection{Evaluationskonzept}
\label{sec:evalkonzept}

\subsubsection{RAGAS}

\textbf{Zielsetzung und methodischer Rahmen}

Zur automatisierten Evaluation des Retrieval-Augmented-Generation (RAG)-Systems wurde das Framework RAGAS in der Version 0.3.8 eingesetzt. RAGAS ist eine spezialisierte Evaluationsbibliothek für RAG-Systeme und ermöglicht die quantitative Bewertung zentraler Qualitätsdimensionen, ohne dass manuell erstellte Referenzantworten erforderlich sind \cite{es2025ragas}. Dadurch eignet sich das Framework insbesondere für Anwendungsszenarien, in denen reale Systemantworten unter praxisnahen Bedingungen analysiert werden sollen.

Die Evaluationsfragen wurden kuratiert und nicht zufällig generiert. Ziel war es, typische Nutzungsszenarien der Plattform systematisch abzubilden. Die insgesamt 60 Fragen umfassen Wissensfragen zu KI-Inhalten, kursbezogene Anfragen, technische Supportfragen sowie Fragen zur Plattformfunktionalität. Grundlage bildeten bestehende Nutzungsmuster des Chatbots sowie eine inhaltliche Analyse ausgewählter Kursmaterialien. 

\textbf{Technische Integration und verwendete Metriken}

Der projektspezifische Ablauf der automatisierten RAGAS-Evaluation ist in Abbildung~\ref{fig:ragas-eval} dargestellt.

\begin{figure}[t]
\centering
\resizebox{0.9\textwidth}{!}{
\begin{tikzpicture}[
  node distance=8mm and 12mm,
  font=\small,
  >=stealth,
  block/.style={draw, rounded corners, align=center, minimum width=9.2cm, minimum height=8mm},
  smallblock/.style={draw, rounded corners, align=center, minimum width=4.4cm, minimum height=8mm},
  note/.style={draw, dashed, rounded corners, align=left, inner sep=5pt, text width=9.2cm},
  arrow/.style={->, line width=0.5pt}
]

\node[block] (q) {Kuratierte Evaluationsfrage \\ ($n=60$)};

\node[smallblock, below left=of q] (v1) {Systemversion A \\ (linear)};
\node[smallblock, below right=of q] (v2) {Systemversion B \\ (routingbasiert)};

\draw[arrow] (q) -- (v1);
\draw[arrow] (q) -- (v2);

\node[block, below=12mm of q] (gen) {Antwortgenerierung durch RAG-System};

\draw[arrow] (v1) -- (gen);
\draw[arrow] (v2) -- (gen);

\node[block, below=of gen] (ctx) {Extraktion der tatsächlich verwendeten \\ \texttt{retrieved\_contexts} (final ausgewählte Kontexte)};

\draw[arrow] (gen) -- (ctx);

\node[block, below=of ctx] (sts) {\texttt{SingleTurnSample} \\ (Frage, Antwort, \texttt{retrieved\_contexts})};

\draw[arrow] (ctx) -- (sts);

\node[block, below=of sts] (eval) {LLM-basierte Bewertung (separates Evaluationsmodell) \\ \texttt{gpt-4o}, Temperatur $=0$};

\draw[arrow] (sts) -- (eval);

\node[block, below=of eval] (metrics) {Berechnung der Metriken \\ \textit{Answer Relevancy} \quad \textit{Context Relevance} \quad \textit{Faithfulness}};

\draw[arrow] (eval) -- (metrics);

\node[block, below=of metrics] (repeat) {Mehrfachausführung: $5$ Wiederholungen je Frage \\ Speicherung der Einzelwerte};

\draw[arrow] (metrics) -- (repeat);

\node[block, below=of repeat] (agg) {Aggregation \\ (Mittelwerte pro Frage und globale Mittelwerte)};

\draw[arrow] (repeat) -- (agg);

\node[block, below=of agg] (out) {Strukturierte Dokumentation der Evaluationsdaten \\ (JSON-Datei)};

\draw[arrow] (agg) -- (out);

\node[note, below=8mm of out] (note) {\textbf{Methodischer Hinweis:} Bewertet werden ausschließlich jene Kontextsegmente, die tatsächlich in die Antwortgenerierung eingeflossen sind. Die Mehrfachausführung reduziert zufällige Schwankungen und erhöht die Stabilität der aggregierten Kennzahlen.};

\end{tikzpicture}
}
\caption{Schematische Darstellung des projektspezifischen Evaluationsprozesses mit RAGAS im Single-Turn-Setting.}
\label{fig:ragas-eval}
\end{figure}

Die technische Implementierung orientiert sich an der von RAGAS vorgesehenen Struktur \texttt{SingleTurnSample} \cite{ragasdocs}. Für jede Evaluationsinstanz werden die Nutzerfrage, die generierte Antwort sowie die tatsächlich verwendeten Kontextsegmente an das Framework übergeben (vgl. Abbildung~\ref{fig:ragas-eval}). Dadurch wird sichergestellt, dass ausschließlich jener Kontext bewertet wird, der tatsächlich in die Antwortgenerierung eingeflossen ist. Die Evaluation bildet somit den realen Antwortprozess des Systems präzise ab.

Als Metriken wurden \textit{Answer Relevancy}, \textit{Context Relevance} und \textit{Faithfulness} eingesetzt. Die Metrik \textit{Answer Relevancy} misst die inhaltliche Passung zwischen Frage und generierter Antwort. \textit{Context Relevance} bewertet die Qualität des Retrievals, indem geprüft wird, inwieweit die abgerufenen Kontexte zur Beantwortung der Frage geeignet sind. \textit{Faithfulness} untersucht, ob die generierte Antwort durch die bereitgestellten Kontexte gestützt ist und somit keine inhaltlich unbegründeten Aussagen enthält. \cite{es2025ragas} Auf Ground-Truth-basierte Metriken wurde bewusst verzichtet, da im Anwendungskontext keine standardisierten Referenzantworten in ausreichender Breite verfügbar waren und der Fokus auf der Bewertung realer Systemantworten lag.

Die Bewertung erfolgte durch ein separates Evaluationsmodell. Verwendet wurde \texttt{gpt-4o} über den \texttt{LangchainLLMWrapper}. Für semantische Ähnlichkeitsberechnungen innerhalb der Metrik \textit{Answer Relevancy} wurden Embeddings des Modells \texttt{text-embedding-3-small} genutzt. Die Temperatur des Evaluationsmodells wurde auf 0 gesetzt, um eine möglichst deterministische Bewertung zu gewährleisten. Gleichwohl ist zu berücksichtigen, dass LLM-basierte Evaluationsmetriken selbst modellabhängig sind, sodass eine gewisse Bewertungsunsicherheit grundsätzlich bestehen bleibt.

Ergänzend wurde ein Prüfmechanismus implementiert, der potenziell ungestützte Antworten identifiziert. Liegt eine hohe \textit{Answer Relevancy} bei gleichzeitig niedriger \textit{Context Relevance} vor, kann dies darauf hindeuten, dass eine Antwort zwar inhaltlich plausibel erscheint, jedoch nicht ausreichend durch die abgerufenen Kontexte gestützt ist. Solche Fälle wurden gesondert markiert und dienen der vertiefenden qualitativen Analyse.

Zur Reduktion zufälliger Schwankungen wurde jede Frage fünfmal beantwortet und bewertet. Die Anzahl von fünf Wiederholungen stellt einen pragmatischen Kompromiss zwischen statistischer Stabilisierung der Ergebnisse und vertretbarem Rechen- sowie Kostenaufwand dar. Die Metrikwerte wurden pro Durchlauf gespeichert und anschließend zu Durchschnittswerten pro Frage sowie zu globalen Durchschnittswerten aggregiert. Alle Evaluationsdaten wurden automatisiert in einer strukturierten JSON-Datei dokumentiert, um einen systematischen Vergleich zwischen der bestehenden und der neuen Version des Chatbots zu ermöglichen.

\textbf{Evaluation der bestehenden und der neuen Systemversion}

In der bestehenden Version basiert die RAG-Pipeline auf einer linearen Architektur. Der Retriever identifiziert relevante Textsegmente, die unmittelbar an das Sprachmodell zur Antwortgenerierung übergeben werden. Für die Evaluation wurden exakt diese Kontexte extrahiert und gemeinsam mit der generierten Antwort an RAGAS übergeben. Dadurch entspricht die Bewertungsgrundlage dem tatsächlichen Systemverhalten.

Die neue Version weist eine komplexere Architektur auf. Vor dem Retrieval erfolgt eine Klassifikation der Anfrage, um zwischen einfachen und komplexen Fragestellungen zu unterscheiden. Abhängig von dieser Einordnung wird entweder ein Single-Hop-Verfahren oder ein Multi-Hop-Ansatz eingesetzt. Bei komplexen Anfragen wird die ursprüngliche Frage in Teilfragen zerlegt, parallel verarbeitet und anschließend zusammengeführt. Die resultierenden Kontexte werden dedupliziert und rerankt. Auch bei einfachen Anfragen erfolgt ein Reranking, um die relevantesten Segmente auszuwählen.

In beiden Versionen wurden ausschließlich die final ausgewählten Kontexte als \texttt{retrieved\_contexts} an RAGAS übergeben. Damit wird sichergestellt, dass die Evaluation jeweils die tatsächlich verwendeten Kontextinformationen berücksichtigt und die Systemarchitektur adäquat abbildet. Unabhängig von der internen Pipeline-Logik beider Versionen bleibt das evaluative Single-Turn-Setting identisch, sodass die Vergleichbarkeit der Ergebnisse gewährleistet ist.

% Warum wurden genau fünf Wiederholungen pro Frage gewählt?
% Wurden API-Rate-Limits, Timeouts oder fehlgeschlagene Calls kontrolliert oder geloggt?
% Bewertet hier ein LLM ein anderes LLM?
% Ist ein Single-Turn-Setting ausreichend für die Bewertung eines Lern-Chatbots?
% Würde eine Multi-Turn-Evaluation zu anderen Ergebnissen führen?

\subsubsection{Blindtest: Experten}

Zur vergleichenden Evaluation der beiden RAG-Systeme wurde ein expertenbasierter Blindtest im A/B-Design durchgeführt. Methodisch orientiert sich das Vorgehen am Arena-Prinzip, bei dem zwei Systemantworten auf denselben Prompt parallel präsentiert werden, ohne dass die Bewertenden wissen, welches System welche Antwort erzeugt hat. Dieses Design reduziert Erwartungs- und Confirmation-Bias und ist insbesondere für generative Systeme geeignet, deren Qualität nicht ausschließlich anhand einer eindeutig definierten Ground Truth beurteilt werden kann. Während automatisierte Metriken wie \emph{Faithfulness} oder faktische Korrektheit die kontextuelle Stützung und inhaltliche Konsistenz quantifizieren \cite{Roychowdhury2024RAGAS}, erlaubt ein Blindvergleich die Bewertung wahrgenommener Gesamtqualität im direkten Systemvergleich.

\begin{figure}[h]
    \centering
    \fbox{\parbox[c][6cm][c]{0.9\textwidth}{\centering Platzhalter für Abbildung: Arena-Screenshot mit A/B-Systemvergleich}}
    \caption{Arena-Setup im Experten-Blindtest: Zwei Systemantworten (A/B) werden parallel im identischen Interface dargestellt.}
    \label{fig:arena}
\end{figure}

Die Untersuchung wurde als moderiertes Einzelinterview durchgeführt (30--60 Minuten pro Person). Insgesamt nahmen 14 Expert*innen teil, darunter Autor*innen von E-Learning-Inhalten, Lehrkräfte sowie Fachpersonen aus dem KI-Campus-Umfeld, aus der angewandten Forschung (Fraunhofer) und aus dem universitären Kontext (Freie Universität Berlin). Diese Zusammensetzung gewährleistet sowohl fachliche als auch didaktische Expertise und erlaubt eine fundierte Beurteilung inhaltlicher Korrektheit, Kontextintegration und Lernförderlichkeit. Die Relevanz solcher mehrdimensionaler Qualitätskriterien im Bildungskontext wird auch in der Literatur zu RAG in Educational Settings hervorgehoben \cite{Li2025RAGEducationSurvey,Swacha2025RAGEducation}. Zudem zeigen Studien zur Nutzung generativer KI im Unterricht, dass didaktische Angemessenheit und die Förderung kritischer Bewertungskompetenz zentrale Qualitätsdimensionen darstellen \cite{Oates2025ChatGPTClassroom}.

\textbf{Bewertungsinstrument}

Die Evaluation erfolgte mittels standardisiertem Fragebogen (Likert-Skala, 1 = trifft nicht zu, 5 = trifft voll zu) sowie begleitender Think-Aloud-Methode. Die Bewertungsdimensionen wurden wie folgt operationalisiert:

\begin{table}[h]
\centering
\caption{Bewertungsdimensionen des Experten-Blindtests}
\label{tab:experten_kriterien}
\begin{tabular}{p{1cm} p{3.5cm} p{7cm}}
\hline
\textbf{Kürzel} & \textbf{Dimension} & \textbf{Bewertungsstatement} \\
\hline
I1 & Inhaltliche Korrektheit & Die Antwort ist fachlich korrekt und enthält keine sachlichen Fehler. \\
I2 & Vollständigkeit & Die Antwort deckt die wesentlichen Aspekte der Frage vollständig ab. \\
I3 & Relevanz & Die Antwort geht präzise auf die gestellte Frage ein und enthält keine irrelevanten Informationen. \\
I4 & Kontextintegration / Kohärenz & Die Antwort integriert relevante Kontextinformationen schlüssig und ist in sich konsistent. \\
I5 & Verständlichkeit / Struktur & Die Antwort ist klar strukturiert, gut verständlich formuliert und sprachlich angemessen. \\
I6 & Lernförderlichkeit / Didaktik & Die Antwort unterstützt das Verständnis des Themas und fördert den Lernprozess. \\
\hline
\end{tabular}
\end{table}

Die Kombination aus quantitativer Skalenbewertung und qualitativer Think-Aloud-Methode ermöglicht eine differenzierte Analyse. Während die Likert-Skalen systematische Vergleiche der beiden Systeme über mehrere Qualitätsdimensionen hinweg erlauben, liefert die verbale Begleitreflexion Einblick in Entscheidungslogiken, Unsicherheiten und Bewertungsmaßstäbe der Expert*innen. Gerade im Bildungskontext ist diese Kombination bedeutsam, da generative KI-Systeme nicht allein anhand faktischer Richtigkeit bewertet werden, sondern insbesondere hinsichtlich ihrer didaktischen Angemessenheit, Strukturierungsleistung und Unterstützung reflektierter Wissensaneignung \cite{Oates2025ChatGPTClassroom,Neumann2024MoodleBot}.

Der Experten-Blindtest ergänzt somit automatisierte RAG-Metriken um eine fachlich-didaktische Validierungsebene. Während quantitative Scores Aussagen über Kontexttreue und faktische Konsistenz erlauben \cite{Roychowdhury2024RAGAS}, erfasst das Arena-Design die wahrgenommene Gesamtqualität im direkten Vergleich – eine zentrale Voraussetzung für die Bewertung konkurrierender RAG-Systeme im Bildungsbereich.

\section{Weiterentwicklung eines Retrieval-Augmented-Generation-Chatbots}

\subsection{Anforderungsanlyse}
Die Weiterentwicklung eines RAG-basierten Chatbots ist ebenso wie dessen Evaluation keine triviale Aufgabe. Um eine zielgerichtete Arbeit sicherzustellen, bedarf es einer systematischen Erfassung der Anforderungen, sodass darauf aufbauend konkrete technische Implementierungsschritte definiert und umgesetzt werden können. In diesem Abschnitt wird die Anforderungsanalyse beschrieben, die als Grundlage für die Weiterentwicklung des Chatbots dient. Zuerst werden allgemeine Anforderungen an RAG-basierte Chatbots auf Basis einer Literaturrecherche identifiziert. Der Fokus liegt hier bereits auf dem Bildungskontext. Darauffolgend werden spezifische Anforderungen für die Weiterentwicklung des Chatbots auf Basis der zentralen Stakeholder definiert. Dazu gehören die Nutzer der Plattform, die den Chatbot verwenden, sowie der KI-Campus als Betreiber der Plattform. Damit wird sichergestellt, dass die Weiterentwicklung des Chatbots nicht nur auf theoretischen Überlegungen basiert, sondern auch die praktischen Bedürfnisse und Erwartungen der Nutzer und des Betreibers berücksichtigt.

\subsubsection{Stand der Forschung}
??
Single Hop und Multi Hop erklären
Hybrides Retrieval erklären
Reranker erklären
?Sokratischer Lernassistent?

\subsubsection{Stakeholder}
INFO ZU USER TESTS ERFRAGEN

Bevor der Durchführung des in Kapitel \ref{sec:evalkonzept} beschriebenen Evaluationskonzepts, erfolgt der Einbezug der Stakeholder primär durch das Auswerten einer ersten Phase von User Tests. Diese wurden im Februar 2025 durch den KI-Campus durchgeführt und folgten den Think-Aloud Protokoll bei Nutzung des Chatbots. Final gesammelt wurden dabei die Defizite der aktuellen Chatbotversion, welche nach Relevanz kategorisiert wurden. Diese Erkenntnisse wurden durch die Autoren in Oberkategorien synthetisiert und diese Ergebnisse sind in Tabelle \ref{tab:usertests} ersichtlich. Im Bereich hoher Relevanz dominieren funkionale Defizite, wie die Antwortlatenz, die Qualität der zurückgegebenen Antworten sowie die Persistenz der Konversationen. Darüber hinaus gibt es Schwierigkeiten im Bereich der Mehrsprachigkeit sowie Bedenken über eine unzureichende Wissensbasis, die eventuell auf externe Wissensquellen durch das Model Context Protocol (MCP) zugreifen sollte. Des weiteren werden Möglichkeiten des Reinforcement Learning from Human Feedback (RLHF) nicht genutzt. Defizite mittlerer Relevanz betreffen vor allem die Bearbeitbarkeit von Chatanfragen, mangelnde Unterstützung informeller Lernprozesse und die fehlende Fähigkeit für Smalltalk. Zudem fehlen multimodale Integrationen. Im Bereich niedriger Relevanz werden neben einem zu simplen Interface, das nicht klar die Funktionalität des Chatbots kommuniziert, vor allem das Fehlen erweiterter Funktionen wie eine Exportfunkiuon oder das Einbeziehen von Community-Informationen kritisiert.
Zu den relevanten Defiziten zählen damit die Kernfunktionalität, Antwortqualität und Kontextverarbeitung, während Erweiterungsfunktionen und Interface-Aspekte eine geringere Priorität aufweisen.

\begin{table}[htbp]
\centering
\renewcommand{\arraystretch}{1.2}

\begin{tabular}{|>{\raggedright\arraybackslash}m{0.3\textwidth}|>{\raggedright\arraybackslash}m{0.3\textwidth}|>{\raggedright\arraybackslash}m{0.3\textwidth}|}
\hline
\multicolumn{1}{|c|}{\textbf{Hoch}} &
\multicolumn{1}{c|}{\textbf{Mittel}} &
\multicolumn{1}{c|}{\textbf{Niedrig}} \\
\hline
Langsames Antworten & Abbrechen von  Chatanfragen fehlt & Simples Interface \\
\hline
Ausbaufähiges Zitieren & Keine personalisierten Antworten & Level der User unbeachtet \\
\hline
Antwortgenauigkeit \& -qualität & Editieren von Chatanfragen fehlt & Zwei Antworten generieren (siehe ChatGPT)? \\
\hline
Mehrsprachiger Support unzureichend & Keine informellen Lernabfragen & Rolle des Chatbots klar benennen \\
\hline
Unzureichende Persistenz der Konversationen & Beispiele und Use Cases in Antwort einbinden & Kein Hinweis auf Funktionalität Chatbots \\
\hline
User Feedback einbinden (RLHF) & Schlechte No-Answer Logik & Keine Exportfunktion der Chats \\
\hline
Größere Wissensbasis für Antwort (MCP?) & Schwache Outputformatierung & Antworten teils generisch und nicht kontextgetreu \\
\hline
Oberflächliche Zusammenfassungen & Multimodale Informationen nicht eingebunden & Keine KIC Community Info im Chatkontext \\
\hline
Follow-up Fragen fehlen &  &  \\
\hline
\end{tabular}
\caption{Festgestellte Defizite nach Relevanz}
\label{tab:usertests}
\end{table}

\subsubsection{Identifizierte Systemanforderungen}
\label{sec:ident_systemanforderungen}
Abbildung \ref{fig:systemanforderungen} systematisiert die aus der Anforderungsanalyse abgeleiteten Verbesserungspotenziale und stellt konkrete Handlungsdimensionen für die Weiterentwicklung des RAG-basierten Chatbots dar. Dazu gehören die Wissensbasis, Vertrauen, Konversationsqualität und Technische Performance. Die Wissensbasis umfasst die Erweiterung des verfügbaren Kontexts des RAG-Systems mittels einen Ausbau der Vektordatenbank und die Erweiterung um multimodale Inhalte sowie das Integrieren von Feedback-Mechanismen (bspw. RLHF). Vertrauen umfasst den im Bildungskontext besonders relevanten Aspekt der Transparenz, der eine nachvollziehbare Zitation und eine klare Rollenbeschreibung des Systems beinhaltet. Die Konversationsqualität umfasst vor allem die Verbesserung der Antwortqualität und der Interaktionsfähigkeit des Chatbots. Hierbei geht es nicht nur darum das einfache "Questions an Answers" (Q\&A) zu verbessern, sondern auch eine stärkere Kontextintegration, die Fähigkeit zu Follow-up-Fragen und die Einbindung von Beispielen und Use Cases als auch die Möglichkeit korrekt zu Smalltalk zu reagieren. Die technische Performance umfasst drei wichtige Kategorien. Die Antwortgeschwindigkeit, die technisch korrekte Funktion des Chatbots, wozu Konversationsperistenz sowie mehrspracheriger Support gehören, und die Verbesserung der User Experience (UX), die vor allem mit einer Verbesserung des User Interface (UI) korrepsondiert. Diese identifizerten Systemanforderungen bilden die Grundlage für die technische Umsetzung der Weiternentwicklung, welche in Kapitel \ref{sec:technische_umsetzung} beschrieben wird.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{systemanforderungen.png}
    \caption{Identifizierte Systemanforderungen}
    \label{fig:systemanforderungen}
\end{figure}

\subsection{Auswahl der Implementierungen}
Aufgrund der zeitlichen Begrenzung der Projektarbeit konnten nicht alle identifizierten Systemanforderungen auch mittels technischer Implementierungen umgesetzt werden. In diesem Kapitel wird beschrieben, welche Anforderungen technisch umgestzt wurden und welche nicht.
Grundlage jedlicher Implementierungen ist stets die bestehende Codebasis, welche im GitHub Repository \textit{kic-web-assistant} \cite{kic_version1} nachvollzogen werden kann. Im Folgenden wird der Code und der dazugehörige Chatbot als Version 1 (V1) bezeichnet. Um bestehende Schwächen sowie Ansatzpunkte für das Umsetzen der Systemanforderungen zu identifizieren, wurde der vorliegende Code analysiert. Zentral ist hierbei nicht die Architektur, welche in Kapitel \ref{sec:kic} bereits beschrieben ist, sondern um die eigentlichen Codezeilen. Hierbei fällt direkt auf, dass Version 1 keine standardisierten Frameworks für die Implementierung nutzt. Jegliche Abläufe sind eigenständig codiert, was die Flexibilität erhöht, aber auch die Wartbarkeit und Erweiterbarkeit erschwert. Zudem sorgt dies auch dafür, dass die aktuelle Persistenz komplett im Frontend der Anwendung liegt und es keine Mechanismen für Persistenz im Backend gibt. Ein weiterer auffälliger Punkt ist die limitierte Ingestion-Pipeline, die nur einen Bruchteil der vorhandenen textbasierten Daten in Moodle verarbeitet. Des Weiteren sind viele der Systemprompts veraltet und beinhalten keine bildungsspezifischen Anweisungen. Die kritisierte No-Answer Logik ist zudem keine Schwäche des verwendeten LLM, sondern hardcodiert in den Workflow des Chatbots. Für eine multimodale Verarbeitung fehlen ebenfalls jegliche Schnittstellen, sodass Kommunikation auf Text limitiert ist. Schließlich konnten verschiedene kleinere Bugs identifiziert werden, die im Laufe der Implementierungen behoben werden können.

Abbildung \ref{fig:auswahl_implementierungen} stellt aus den identifizierten Systemanforderungen ausgewählten Implementierungen dar. Bezüglich der Wissensbasis wird der Ausbau der Vektordatenbank vorangetrieben. In der Codeanalyse wurde die limitierte Vektordatenbank erkannt und soll im Rahmen der Implementierungen erweitert werden. Dafür sollen zum einen deutlich mehr textbasierte Daten eingelesen werden. Zudem wird ein multimodaler Support eingebaut, der vorerst nur Audiodateien umfasst. Eine breitere Verarbeitung von Bildern und Videos ist technisch aufwendig und wurde im Rahmen dieses Projekts nicht priorisiert. Dies gilt auch für das Einbinden von Feedback, was für die unmittelbar folgende Implementierung erstmal keinen Mehrwert bietet und daher ebenfalls nicht priorisiert wurde. Für die Vertrauensdimension spielt die Zitation eine zentrale Rolle. Diese wird vor allem in Blick auf die Transparenz verbessert, während eine Umstellung der Logik die Rollen des Chatbots klarer kommuniziert. In Bezug auf die Konversationsqualität soll vor allem die Umstellung der Logik die identifizierten Systemanforderungen erfüllen, wobei auch das Anpassen der Systemprompts eine wichtige, wenn auch untergeordnete, Rolle spielt. Im Bezug auf die technische Performance soll die Antwortgeschwindigkeit durch die Implementierung von Streaming verbessert werden. Streaming bedeutet, dass die Antwort bereits während der Generierung an den Nutzer zurückgegeben wird, anstatt erst nach vollständiger Generierung. Dies kann die wahrgenommene Antwortlatenz erheblich reduzieren. Die Persistenz im Backend soll durch die Umstellung der Logik sichergestellt werden. Der mehrsprachige Support, obwohl für eine europäische Organisation wie den KI-Campus sehr wichtig, wurde in Absprache mit diesem erstmal nicht priorisiert, unter anderem auch weil die meisten Nutzer der Plattform Deutsch sprechen. Schließlich wurden Änderungen am User Interface weggelassen, weil das Frontend durch einen weiteren Serviceanbieter (LOOM GmbH) verwaltet wird, die nicht Teil des Projekts sind.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{auswahl_implementierungen.png}
    \caption{Darstellung der ausgewählten Systemanforderungen mit konkreter Umsetzung (Auswahl fett markiert)}
    \label{fig:auswahl_implementierungen}
\end{figure}

\subsection{Technische Umsetzung}
\label{sec:technische_umsetzung}

\subsubsection{Erweiterung der Ingestion-Pipeline}
Veit

\subsubsection{Umstellung der Logik}
Die Umstellung der Logik des Chatbots von dem in Kapitel \ref{sec:kic} dargestellten Prompt Chaining hin zu den in Abbildung \ref{fig:worklow_neu} dargestellten Routing-Ansatz stellt die größte Implementierung der Autoren dar. Relevant ist neben der architektonischen Änderungen hier auch das Nutzen der standardisierten Bibliothek LangGraph. Diese erlaubt das einfache Modellieren komplexer Workflows mittels Knoten, in denen die eigentliche Logik codiert ist, und Kanten, die die Verbindungen zwischen den Knoten darstellen. In jedem Knoten wird der State einer Anfrage verändert, welche durch das Durchlaufen aller Knoten komplett verarbeitet wird. Dies ermöglicht eine deutlich leichter erweiterbare und wartbare Architektur und hat eine built-in Funktion für die Persistenz der Konversationen. In Bezug auf Abbildung \ref{fig:worklow_neu} erkennt man leicht, warum der Ansatz Routing genannt wird. Je nach Anfrage werden verschiedene Pfade durch den Workflow genommen, was eine deutlich flexiblere Verarbeitung von Anfragen ermöglicht. Die möglichst optimale Verarbeitung jeder Anfrage ist die Grundlage des Routings. Der Workflow bestimmt dabei auch die Rolle des Chatbots, welche dann eindeutig mit dem Nutzer kommuniziert werden kann. Die ersten beiden Schritte jeder Anfrage sind identisch. Zuerst wird die Anfrage klassifiziert, um zu bestimmen, welcher Workflow genutzt werden sollte. Darauf folgt die bekannte Kontextualisierung. Ab hier gibt es die Aufteilung. Der \textit{Path ohne Retrieval} wird für einfache Anfragen genutzt, die ohne zusätzlichen Kontext beantwortet werden können. Hierzu zählt Smalltalk aber auch die Verarbeitung von sinnlosen Anfragen. Was zuvor mittels der starren No-Answer Logik gelöst wurde, wird nun durch flexible, kontextgetreue Antworten ersetzt. Damit wird schlussendlich auch Kosten gespart, weil unnötige Retrievals vermieden werden. \textit{Single- und Multi Hop} sind die Erweiterung des ursprünglichen linearen Workflows. Einfache Anfragen entsprechen dem Single Hop, weil diese nur eine Single Source of Truth benötigen. Multi Hop sind komplexere Anfragen, die mehrere Kontextquellen benötigen. Hierfür werden Anfragen in mehrere kleinere Teilfragen zerlegt, die parallel als Single Hop verarbeitet werden. Das semantische Retrieval wird zudem durch das hybride Retreival ersetzt. Im Multi Hop Workflow werden hier die zeitgleich retrievden Kontexte dedupliziert. Anschließend kommen alle retrievden Kontexte in einen Reranker, um die Qualität dieser sicherzustellen. Erst danach werden die finalen Kontexte an das LLM übergeben, um die Antwort zu generieren. Die größte Implementierung bei der Umstellung der Logik ist das Einführen des \textit{Sokratischen Lernassistenten}. Dieser wird durch einen hardcodierten Satz (\"Ich möchte etwas Lernen") aktiviert und dient vor allem der Umstellung von einem einfachen Q\&A Chatbot hin zu einer interaktiven Lernunterstützung. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{workflow_neu.png}
    \caption{Neuer Workflow (Routing) des RAG-basierten Chatbots in LangGraph}
    \label{fig:worklow_neu}
\end{figure}

Der schematische Ablauf des sokratischen Lernassistenten ist in Abbildung \ref{fig:socratic} dargestellt. Dieser besteht aus sechs Modi die teilweise linear und teilweise iterativ durchlaufen werden. Nach der Aktivierung durch den Nutzer beginnt der Lernassistent mit dem \textit{Socratic Contracting}. Dies ist ein fester Prompt der den Sinn des Lernmodus kommuniziert und die Regeln der Interaktion festlegt. Der Nutzer wird aufgefordert das Thema zu nennen, das er lernen möchte. Darauf folgt die \textit{Socratic Diagnose}. Dieser Schritt hat das Ziel die Lernziele des Nutzers festzustellen. Man spricht nach Abschluss dieser Stufe von einem entstandenen sokratischen Vertrag, der durch die folgenden Schritte iterativ abgearbeitet wird, sodass der Nutzer am Ende das gewünschte Thema gelernt hat. Diese iterative Stufe ist der \textit{Socratic Core}. In diesem werden stets die sokratischen Fragen gestellt, die die Grundlage des sokratischen Lernens darstellen. Im Idealfall bringt dies den Nutzer dazu das gewünschte Wissen im Dialog durch Follow-up Fragen, informelle Wissensabfrage und Beispiele eigenständig zu erarbeiten. Wenn der Lernassistent den Lernerfolg erkennt, folgt das \textit{Socratic Feedback}, also das Einordnen der Lernleistung, was den Abschluss des Lernmodus darstellt. Sollte der Nutzer feststecken, kann der Lernassistent durch \textit{Socratic Hinting} Hinweise bekommen oder einfach eine Erklärung per \textit{Socratic Explain} erhalten, die beide zum Socratic Core gezählt werden. Der Lernmodus kann dabei nach Abschluss neu gestartet werden oder jederzeit mit oder ohne Erklärung verlassen werden. Die Entscheidungen im Socratic Core werden per Router (Klassifikation) getroffen, während die anderen Schritte hardcodierte Abfolgen sind.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{socratic.png}
    \caption{Schematischer Ablauf des sokratischen Lernassistenten}
    \label{fig:socratic}
\end{figure}

\subsubsection{Weitere Anpassungen}
Neben der Erweiterung der Ingestion-Pipeline und der Umstellung der Logik wurden drei weitere, kleinere Anpassungen durchgeführt, die an dieser Stelle kurz beschrieben werden.

Dies ist zum einen die \textbf{Anpassung der Systemprompts} Max?!

Zum anderen gibt es ein \textbf{Anpassen der Zitation} mit dem Hauptziel die verwendeten Quellen transparenter darzustellen. Anstatt in der Antwort einfach nur eine Zahl stehen zu haben (\textit{[1]}) wird der Titel es jeweiligen Moodle-Moduls angezeigt (\textit{[Titel der Quelle]}). Besonders durch den starken Anstieg der eingelesenen Module und damit verbunden deutlich mehr möglichen Quellen ermöglicht dies bereits in der Antwort abschätzen zu können, ob eine Quelle sinnvoll ist und nicht erst dem verfügbaren Link folgen zu müssen. Der implementierte Reranker sorgt zudem dafür, dass die Qualität der zurückgegebenen Quellen verbessert wird, was die Zitation zusätzlich verbessert.

Abschließend wurde die \textbf{Implementierung von Streaming} durchgeführt. Hierfür wird jeder Token direkt bei der Generierung an das Frontend zurückgegeben und damit direkt für den Nutzer sichtbar gemacht. Nach Abschluss der Antwortgenerierung greift das Postprocessing trotzdem und erstellt wie gehabt die verwendeten Quellen, um diese am Ende der Antwort anzuzeigen.